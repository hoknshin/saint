{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ccc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm saint -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751362ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'saint'...\n",
      "remote: Enumerating objects: 70, done.\u001b[K\n",
      "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 70 (delta 18), reused 16 (delta 16), pack-reused 41\u001b[K\n",
      "Unpacking objects: 100% (70/70), 211.22 KiB | 1.04 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hoknshin/saint.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13eac50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('saint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61748045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE\t\t  data_openml.py  pipeline.png\t\t train.py\n",
      "README.md\t  models\t  pretraining.py\t train_robust.py\n",
      "augmentations.py  old_version\t  saint_environment.yml  utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ebef5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting einops\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m93.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting openml\n",
      "  Downloading openml-0.13.1.tar.gz (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.6/127.6 kB\u001b[0m \u001b[31m39.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting liac-arff>=2.4.0\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting xmltodict\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from openml) (2.28.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.8/dist-packages (from openml) (0.24.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from openml) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from openml) (1.4.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.8/dist-packages (from openml) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from openml) (1.22.2)\n",
      "Collecting minio\n",
      "  Downloading minio-7.1.14-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.2/77.2 kB\u001b[0m \u001b[31m33.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (from openml) (8.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->openml) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil->openml) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->openml) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->openml) (3.1.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from minio->openml) (1.26.12)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from minio->openml) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->openml) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->openml) (2.1.1)\n",
      "Building wheels for collected packages: openml, liac-arff\n",
      "  Building wheel for openml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openml: filename=openml-0.13.1-py3-none-any.whl size=142800 sha256=14b7fc920731a3c12d06fcfe6c2806263a6f80141c15ab1a565d3723413548e6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2u52k_wp/wheels/c4/1c/5e/5775d391b42f19ce45a465873d8ce87da9ea56f0cd3af920c4\n",
      "  Building wheel for liac-arff (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11732 sha256=f98c72f066b1a8ecd62e93808c19974d8594ae2637c0a67ac150ffbd06406890\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2u52k_wp/wheels/a2/de/68/bf3972de3ecb31e32bef59a7f4c75f0687a3674c476b347c14\n",
      "Successfully built openml liac-arff\n",
      "Installing collected packages: xmltodict, minio, liac-arff, einops, openml\n",
      "Successfully installed einops-0.6.0 liac-arff-2.5.0 minio-7.1.14 openml-0.13.1 xmltodict-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install einops openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be480e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Jun 22 2022, 20:18:18) \n",
      "[GCC 9.4.0]\n",
      "1.13.0a0+936e930\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "921bd9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda:0.\n",
      "Downloading and processing the dataset, it might take some time.\n",
      "50 256\n",
      "Namespace(active_log=False, attention_dropout=0.8, attention_heads=4, attentiontype='colrow', batchsize=256, cont_embeddings='MLP', dset_id=41150, dset_seed=5, dtask='clf', embedding_size=32, epochs=100, ff_dropout=0.8, final_mlp_style='sep', lam0=0.5, lam1=10, lam2=1, lam3=10, lr=0.0001, mask_prob=0, mixup_lam=0.3, nce_temp=0.7, optimizer='AdamW', pretrain=True, pretrain_epochs=50, pt_aug=[], pt_aug_lam=0.1, pt_projhead_style='diff', pt_tasks=['contrastive', 'denoising'], run_name='testrun', savemodelroot='./bestmodels', scheduler='cosine', set_seed=1, ssl_avail_y=0, task='binary', train_mask_prob=0, transformer_depth=1, vision_dset=False)\n",
      "Pretraining begins!\n",
      "Epoch: 0, Running Loss: 604.0518712326884\n",
      "Epoch: 1, Running Loss: 166.91892969235778\n",
      "Epoch: 2, Running Loss: 135.75879099499434\n",
      "Epoch: 3, Running Loss: 153.53852261509746\n",
      "Epoch: 4, Running Loss: 122.80139843188226\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 158, in <module>\n",
      "    model = SAINT_pretrain(model, cat_idxs,X_train,y_train, continuous_mean_std, opt,device)\n",
      "  File \"/root/saint/pretraining.py\", line 89, in SAINT_pretrain\n",
      "    optimizer.step()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\", line 140, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py\", line 133, in step\n",
      "    if p.grad is None:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dset_id 41150 --task binary --pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a1d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087104b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
