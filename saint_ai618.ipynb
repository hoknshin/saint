{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51b4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm saint -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae459527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'saint'...\n",
      "remote: Enumerating objects: 70, done.\u001b[K\n",
      "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 70 (delta 18), reused 16 (delta 16), pack-reused 41\u001b[K\n",
      "Unpacking objects: 100% (70/70), 211.22 KiB | 1.04 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hoknshin/saint.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebe6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('saint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54aac823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE\t\t  bestmodels\t  pipeline.png\t\t train.py\n",
      "README.md\t  data_openml.py  pretraining.py\t train_robust.py\n",
      "__pycache__\t  models\t  saint_ai618.ipynb\t utils.py\n",
      "augmentations.py  old_version\t  saint_environment.yml\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fddc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting einops\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m93.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting openml\n",
      "  Downloading openml-0.13.1.tar.gz (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.6/127.6 kB\u001b[0m \u001b[31m39.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting liac-arff>=2.4.0\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting xmltodict\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from openml) (2.28.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.8/dist-packages (from openml) (0.24.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from openml) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from openml) (1.4.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.8/dist-packages (from openml) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from openml) (1.22.2)\n",
      "Collecting minio\n",
      "  Downloading minio-7.1.14-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.2/77.2 kB\u001b[0m \u001b[31m33.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (from openml) (8.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->openml) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil->openml) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->openml) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->openml) (3.1.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from minio->openml) (1.26.12)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from minio->openml) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->openml) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->openml) (2.1.1)\n",
      "Building wheels for collected packages: openml, liac-arff\n",
      "  Building wheel for openml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openml: filename=openml-0.13.1-py3-none-any.whl size=142800 sha256=14b7fc920731a3c12d06fcfe6c2806263a6f80141c15ab1a565d3723413548e6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2u52k_wp/wheels/c4/1c/5e/5775d391b42f19ce45a465873d8ce87da9ea56f0cd3af920c4\n",
      "  Building wheel for liac-arff (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11732 sha256=f98c72f066b1a8ecd62e93808c19974d8594ae2637c0a67ac150ffbd06406890\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2u52k_wp/wheels/a2/de/68/bf3972de3ecb31e32bef59a7f4c75f0687a3674c476b347c14\n",
      "Successfully built openml liac-arff\n",
      "Installing collected packages: xmltodict, minio, liac-arff, einops, openml\n",
      "Successfully installed einops-0.6.0 liac-arff-2.5.0 minio-7.1.14 openml-0.13.1 xmltodict-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install einops openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122b7876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Jun 22 2022, 20:18:18) \n",
      "[GCC 9.4.0]\n",
      "1.13.0a0+936e930\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b546a859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda:0.\n",
      "Downloading and processing the dataset, it might take some time.\n",
      "50 256\n",
      "Namespace(active_log=False, attention_dropout=0.8, attention_heads=4, attentiontype='colrow', batchsize=256, cont_embeddings='MLP', cov_coeff=1, dset_id=41150, dset_seed=5, dtask='clf', embedding_size=32, epochs=100, ff_dropout=0.8, final_mlp_style='sep', lam0=0.5, lam1=0, lam2=0, lam3=25, lr=0.0001, mask_prob=0, mixup_lam=0.3, nce_temp=0.7, optimizer='AdamW', pretrain=True, pretrain_epochs=50, pt_aug=[], pt_aug_lam=0.1, pt_projhead_style='diff', pt_tasks=['contrastive', 'denoising'], run_name='testrun', savemodelroot='./bestmodels', scheduler='cosine', set_seed=1, ssl_avail_y=0, std_coeff=25, task='binary', train_mask_prob=0, transformer_depth=1, vision_dset=False)\n",
      "Pretraining begins!\n",
      "std loss 0.9829 cov loss 0.0000\n",
      "std loss 0.9812 cov loss 0.0000\n",
      "std loss 0.9788 cov loss 0.0000\n",
      "std loss 0.9772 cov loss 0.0000\n",
      "std loss 0.9753 cov loss 0.0000\n",
      "std loss 0.9757 cov loss 0.0000\n",
      "std loss 0.9738 cov loss 0.0000\n",
      "std loss 0.9715 cov loss 0.0000\n",
      "std loss 0.9720 cov loss 0.0000\n",
      "std loss 0.9702 cov loss 0.0000\n",
      "std loss 0.9684 cov loss 0.0000\n",
      "std loss 0.9655 cov loss 0.0001\n",
      "std loss 0.9616 cov loss 0.0002\n",
      "std loss 0.9607 cov loss 0.0002\n",
      "std loss 0.9616 cov loss 0.0001\n",
      "std loss 0.9588 cov loss 0.0002\n",
      "std loss 0.9563 cov loss 0.0003\n",
      "std loss 0.9495 cov loss 0.0009\n",
      "std loss 0.9479 cov loss 0.0011\n",
      "std loss 0.9454 cov loss 0.0016\n",
      "std loss 0.9383 cov loss 0.0034\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 163, in <module>\n",
      "    model = SAINT_pretrain(model, cat_idxs,X_train,y_train, continuous_mean_std, opt,device)\n",
      "  File \"/root/saint/pretraining.py\", line 117, in SAINT_pretrain\n",
      "    optimizer.step()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\", line 140, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py\", line 162, in step\n",
      "    adamw(params_with_grad,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py\", line 219, in adamw\n",
      "    func(params,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py\", line 318, in _single_tensor_adamw\n",
      "    param.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dset_id 41150 --task binary --pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3333c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c920d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
